{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-5. CNN Ex3-CIFAR 10_v2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPp2fpOeb+qK9vaosOHb6Hp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jong9810/TensorFlow-2.0/blob/main/5_5_CNN_Ex3_CIFAR_10_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Ex3 : CIFAR 10 (7 Conv layer, 5 Pooling layer)"
      ],
      "metadata": {
        "id": "n6WTTJPqt3dJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "I1W-fXjzuA0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, t_train), (x_test, t_test) = cifar10.load_data()\n",
        "\n",
        "print(x_train.shape, t_train.shape)\n",
        "print(x_test.shape, t_test.shape)"
      ],
      "metadata": {
        "id": "mAGoOoULuA39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(t_train[0])"
      ],
      "metadata": {
        "id": "f8BOLu01vMAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype(np.float32) / 255.0\n",
        "x_test = x_test.astype(np.float32) / 255.0"
      ],
      "metadata": {
        "id": "pmtwEPOGuA7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "V4UZUGd2wfrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(np.random.randint(3, size=7)) # np.random.randint(3, size=7) : 0 ~ 2 까지 숫자를 7개의 원소를 가지는 배열로 반환해줌"
      ],
      "metadata": {
        "id": "JcfYDr8ezmAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = np.array([1, 2, 3])\n",
        "# b = np.array([4, 5])\n",
        "# print(np.concatenate( (a, b) ) ) # np.concatenate( (a, b) ) : numpy 배열 a, b가 있을 때 a의 뒤에 b를 이어붙임"
      ],
      "metadata": {
        "id": "HrIvTqDE0S0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c = np.arange(5) # np.arange(5) : 각각의 원소가 자신의 인덱스 수인 5개의 원소를 가지는 numpy 배열을 생성\n",
        "# print(c)\n",
        "# np.random.shuffle(c) # numpy 배열 c에 대해 모든 원소의 위치를 랜덤으로 재배열\n",
        "# print(c)"
      ],
      "metadata": {
        "id": "9UHYiETE09aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR 10 데이터 보강 (150% 증대)\n",
        "# 30도 범위에서 회전, 20% 범위에서 기울임, 20% 범위에서 가로, 세로방향 이동, 상하 반전 가능\n",
        "gen = ImageDataGenerator(rotation_range=30, shear_range=0.2, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n",
        "\n",
        "augment_ratio = 1.5 # 전체 데이터의 150%\n",
        "augment_size = int(augment_ratio * x_train.shape[0]) # \n",
        "\n",
        "# x_train.shape[0] = 50000, auqment_size = 75000\n",
        "randidx = np.random.randint(x_train.shape[0], size=augment_size) # 0 ~ 49999 까지의 수 중 랜덤으로 75000개 숫자를 반환하여 배열 생성 (중복 가능)\n",
        "\n",
        "# .copy()함수를 사용하는 이유 : 원본 데이터를 그대로 사용하면 원본 데이터 훼손 가능성이 있기 때문\n",
        "x_augmented = x_train[randidx].copy()\n",
        "t_augmented = t_train[randidx].copy()\n",
        "\n",
        "x_augmented, t_augmented = gen.flow(x_augmented, t_augmented, batch_size=augment_size, shuffle=False).next()\n",
        "\n",
        "# 기존의 학습 데이터(50000개)에 보강된 데이터(75000개)를 이어 붙임 ->> 총 125000개 학습 데이터\n",
        "x_train = np.concatenate( (x_train, x_augmented) )\n",
        "t_train = np.concatenate( (t_train, t_augmented) )\n",
        "\n",
        "# 보강된 학습 데이터, 정답 데이터를 기존의 데이터와 랜덤하게 섞음\n",
        "s = np.arange(x_train.shape[0]) # x_train.shape[0] = 125000,  s = [0, 1, 2, ..., 124998, 124999]\n",
        "np.random.shuffle(s)\n",
        "# print(s)\n",
        "\n",
        "# 학습 데이터의 순서를 위에서 정의한 s 배열의 원소의 순서대로 섞어줌\n",
        "x_train = x_train[s]\n",
        "t_train = t_train[s]"
      ],
      "metadata": {
        "id": "ijfzDjdcwfvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = Sequential()\n",
        "\n",
        "cnn.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', input_shape=(32,32,3))) # cifar10텐서 (높이, 너비, 채널)\n",
        "cnn.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn.add(Dropout(0.25))\n",
        "\n",
        "cnn.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "cnn.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn.add(Dropout(0.25))\n",
        "\n",
        "cnn.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn.add(Dropout(0.25))\n",
        "cnn.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn.add(Dropout(0.25))\n",
        "\n",
        "cnn.add(Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
        "cnn.add(Dropout(0.25))\n",
        "\n",
        "cnn.add(Flatten()) # 3차원 텐서를 1차원 벡터로 변환\n",
        "cnn.add(Dense(128, activation='relu')) # 은닉층 개념\n",
        "cnn.add(Dropout(0.5))\n",
        "cnn.add(Dense(10, activation='softmax')) # 출력층"
      ],
      "metadata": {
        "id": "cHu25MievtFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "INu3HAl0xOdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(metrics=['accuracy'], loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
        "cnn.summary()"
      ],
      "metadata": {
        "id": "dLpEI9LZuBCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = cnn.fit(x_train, t_train, validation_data=(x_test, t_test), epochs=250, batch_size=256)"
      ],
      "metadata": {
        "id": "bHW2Yu-duBGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.evaluate(x_test, t_test)"
      ],
      "metadata": {
        "id": "Kb2zDcL0uBId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history['loss'], label='trian')\n",
        "plt.plot(hist.history['val_loss'], label='validation')\n",
        "\n",
        "plt.title('Loss Trend')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.grid()\n",
        "plt.lengend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lu93_NicuBLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history['accuracy'], label='trian')\n",
        "plt.plot(hist.history['val_loss'], label='validation')\n",
        "\n",
        "plt.title('Accuracy Trend')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.grid()\n",
        "plt.lengend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VLZfXQ1TuBOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CIFAR 10_v1과 결과를 비교해보면\n",
        "1. v1 (2 Conv layer, 1 Polling layer)\n",
        "- epoch이 증가함에 따라 오버피팅은 정확도 70% 이후에 계속해서 증가했음\n",
        "\n",
        "2. v2 (7 Conv layer, 5 Pooling layer)\n",
        "- epoch이 증가함에 따라 오버피팅은 정확도는 87%로 거의 일정했고 오버피팅은 거의 발생하지 않았다는 것을 알 수 있었음\n",
        "\n",
        "3. 결론\n",
        "- v1에 비교해서 v2에서는 데이터 보강과 CNN 모델의 층을 더 복잡하게 구성하였다\n",
        "- 그 결과 정확도는 증가하였고 오버피팅은 감소하였다. \n",
        "- v1 정확도 : 70%, v2 정확도 : 87%\n",
        "- v1 : 오버피팅이 epoch이 증가할 수록 증가, v2 : 오버피팅이 거의 발생하지 않음\n",
        "\n",
        "- 하지만 데이터 보강을 해서 데이터의 수가 배로 증가하였고, 층도 더 많았기 때문에 학습하는 데 걸리는 시간도 증가한다는 것을 알았다. (v1 : 1시간 40분 정도, v2 : )\n",
        "\n",
        "- 따라서 CNN 모델을 구성할 때에는 목표로 하는 정확도와 얼마만큼의 시간을 쓸 수 있는지에 따라 적절하게 층을 구성하는 것이 중요할 것이다."
      ],
      "metadata": {
        "id": "1yKrGY6QuBRt"
      }
    }
  ]
}